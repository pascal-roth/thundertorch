{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning Models\n",
    "================\n",
    "\n",
    "The NN models are the \"black box\", the part that learns the patterns in the data while training is performed. The given toolkit allows an easy generation of the most common networks via a direct code implementation or by using the yaml interfaces. In general are all models build by using the PyTorch Lightning Wrapper that removes the boilerplate of the PyTorch models without limiting the flexibility. This tutorial will present the general structure/ possibilities of the implemented models. However, for more detailed information please have a look at the documentation of the single models and the [PyTorch Lightning Docu](https://pytorch-lightning.readthedocs.io/en/0.7.6/lightning-module.html). The toolbox has pre-implemented a FlexMLP and a more general FlexNN that can be used for CNNs, RNNs, and MLPs. Especially for the FelxNN, the usage of the yaml interface is recommended in order to have maximal possible structure. Own models can be defined using the LightningModelTemplate.py file.\n",
    "\n",
    "It has to be mentioned that the default dtype of pytorch is changed to double. This change is necessary in order to export models to C++. As a consequence, also the input has to be of dtype double. \n",
    "\n",
    "Initialization methods\n",
    "----------------------\n",
    "A Model can be initialized by a direct code implementation or by using the yaml structure. Thereby, each implemented Model has a yaml template saved as a staticmethod. In order to employ the yaml file, the utils functions can be used. The check_argsModel(args_yaml) function is not mandatory, however, the usage is recommended in order to detect possible errors and secure that the intended output is provided. In the following the yaml approach is demonstrated using the LightingFlexMLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the yaml_template\n",
    "\n",
    "import yaml\n",
    "from thunder.utils import *\n",
    "\n",
    "args_yaml = parse_yaml('path.yaml')\n",
    "check_argsModel(args_yaml['model'])\n",
    "model = get_model(argsModel['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a Model is initialized by a direct code implementation, it requires a Namespace object as input. This object contains all required hyperparameter used to construct the network, as well as set activation, loss, and optimization function. Three different ways can be identified in order to create Namespace objects:\n",
    "\n",
    "1. Create empty Namespace object and add arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from thunder import models\n",
    "\n",
    "hparams = Namespace()\n",
    "hparams.key_1 = 'key_value'\n",
    "hparams.key_2 = 'int/float/str/dict'\n",
    "\n",
    "model = models.LightningFlexMLP(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Convert a dict to a Namespace object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "hparams_dict = {'key_1': 'key_value', 'key_2': 'int/float/str/dict'}\n",
    "hparams = Namespace(**hparams_dict)\n",
    "\n",
    "model = models.LightningFlexMLP(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Parse arguments using Namespace parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "hparams_parser = argparse.ArgumentParser()\n",
    "hparams_parser.add_argument('--key_1', type=str, default='key_value')\n",
    "hparams_parser.add_argument('--key_2', type=str, default='int/float/str/dict')\n",
    "hparams = hparams_parser.parse_args()\n",
    "\n",
    "model = models.LightningFlexMLP(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Hyperparameters\n",
    "---------------------\n",
    "Two classes of hyperparameters can be identified: model and algorithm hyperparameters. The model hyperparameters are used in the model construction task and thus are not inferred while fitting the network. However, these parameters influence the learning capability and have to be adjusted in case the model complexity is chosen differently compared to the complexity of the addressed problem. In the toolbox, theses model hyperparameters are unique for each model and do not have default values. As an example the model hyperparameters of the LightningFlexMLP are:\n",
    "\n",
    "| key                    | dtype       | description                                                                    |\n",
    "|------------------------|-------------|--------------------------------------------------------------------------------|\n",
    "| n_inp:                 | int         | Input dimension (required)                                                     |\n",
    "| n_out:                 | int         | Output dimension (required)                                                    |\n",
    "| hidden_layer:          | list        | List of hidden layers with number of hidden neurons as layer entry (required)  |\n",
    "\n",
    "\n",
    "Algorithm hyperparameters, in theory, do not influence the model performance, instead, they impact the speed and quality of the learning process. In practice, however, algorithm hyperparameters do influence the capability and they have to be optimized. Algorithm hyperparameters are similar in each model and normally have default values. The toolbox algorithm hyperparameters are:\n",
    "\n",
    "| key                    | dtype       | description                                                                    |\n",
    "|------------------------|-------------|--------------------------------------------------------------------------------|\n",
    "| activation:            | str         | activation fkt that is included in torch.nn (default: ReLU) |\n",
    "| loss:                  | str         | loss fkt that is included in torch.nn (default: MSELoss) |\n",
    "| optimizer:             | dict        | dict including optimizer fkt type and possible parameters, optimizer has to be included in torch.optim (default: {'type': Adam, 'params': {'lr': 1e-3}}) |\n",
    "| scheduler:             | dict        | dict including execute flag, scheduler fkt type and possible parameters, scheduler has to be included in torch.optim.lr_scheduler (default: {'execute': False}) |\n",
    "| num_workers:           | int         | number of workers in DataLoaders (default: 10) |\n",
    "| batch:                 | int         | batch size of DataLoaders (default: 64) |\n",
    "| output_activation:     | str         | torch.nn activation fkt at the end of the last layer (default: None) |\n",
    "\n",
    "\n",
    "\n",
    "Each model has to functions in order to save hyperparameter to a yaml file and update hyperparameters. In the following is a short code example shown that employs the LightningFlexMLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thunder import models\n",
    "\n",
    "model = models.LightningFlexMLP(hparams)\n",
    "\n",
    "# update hparams by dict or Namespace\n",
    "update_hparam = {'loss': RelativeMSELoss, 'optimizer': {'type': 'SGD', 'params': {'lr': 1e-3}}}\n",
    "model.hparams_update(update_hparam)\n",
    "\n",
    "# save hparams to yaml file\n",
    "model.hparams_save('some_path.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightningModelBase and Individual Models\n",
    "----------------------------------------\n",
    "\n",
    "The Toolbox has an own ModelBase class which contains the repeading functions like training, validation and test step. This ModelBase Class furthermore has two functionalities that can construct most of the network layers which are included in torch.nn so that most models can be constructed by just using these functions. \n",
    "\n",
    "However, if the addressed task cannot be solved using the pre-implemented methods, individual modules can be constructed and then used instead. A detailed explanation on how to include individual models in the toolbox can be found [here](./Individualized_modules). It is important to keep in mind that the functions defined in the thunder.model._LightningModelTemplate.LightningModelTemplate have to be included since they will be used in the training procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Extensions\n",
    "-----------------\n",
    "Model functionalities can be extended using different callbacks or metrics. Detailed explanations can be found here:\n",
    "\n",
    "- [Callbacks](./Callbacks.html)\n",
    "- [Metrics](./Metrics.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
