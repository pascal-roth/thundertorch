{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics\n",
    "=======\n",
    "\n",
    "Evaluating the machine learning algorithm is an essential part of any project. The implemented model may give satisfying results when evaluated using a metric say accuracy_score but may give poor results when evaluated against other metrics such as logarithmic_loss or any other such metric. Often one metric such as accuracy for classification tasks is used to measure the performance of our model, however it is not enough to truly judge our model. As a consequence, the latest versions of PyTorch Lightning have a metrics API, which can basically also be used with the toolbox version of lightning (limited due to cluster support for PyTorch only till version 1.2.0). A detailed documentation of the Metrics API can be found [here](https://pytorch-lightning.readthedocs.io/en/latest/metrics.html). \n",
    "\n",
    "A minimal example of an accuracy metrics is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thunder.metrics.metric import Metric\n",
    "\n",
    "class MyAccuracy(Metric):\n",
    "    def __init__(self, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "\n",
    "        self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        preds, target = self._input_format(preds, target)\n",
    "        assert preds.shape == target.shape\n",
    "\n",
    "        self.correct += torch.sum(preds == target)\n",
    "        self.total += target.numel()\n",
    "\n",
    "    def compute(self):\n",
    "        return self.correct.float() / self.total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage in the Toolbox**\n",
    "\n",
    "The Toolbox already included the metric base class of Lightning and the corresponding utils functions. In order to use the pre-implemented metrics, the source code has to be copied into the metrics directory of the ML_Utils toolbox. As an example, this has been made with the \"Explained Variance\" metric.\n",
    "\n",
    "Metrics usage\n",
    "-------------\n",
    "The metrics can be employed by implementing the different steps directly in the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    ...\n",
    "    self.accuracy = pl.metrics.Accuracy()\n",
    "\n",
    "def training_step(self, batch, batch_idx):\n",
    "    logits = self(x)\n",
    "    ...\n",
    "    # log step metric\n",
    "    self.log('train_acc_step', self.accuracy(logits, y))\n",
    "    ...\n",
    "\n",
    "def training_epoch_end(self, outs):\n",
    "    # log epoch metric\n",
    "    self.log('train_acc_epoch', self.accuracy.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is recommended to implement the metric as a **Callback**. Therefore, the model stays clean and different metrics can be activated just by changing a key in the yaml template. An example how to implement a metric as callback can be found [here](./Callbacks.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
