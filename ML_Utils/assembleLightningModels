import stfs_pytoolbox.ML_Utils.models
from stfs_pytoolbox.ML_Utils.models import *
from stfs_pytoolbox.ML_Utils.utils.general import load_model_from_checkpoint
import numpy as np
import torch
import argparse


def parseArguments():
    parser = argparse.ArgumentParser("This script assembles multiple model created by the stfs_pytoolbox.ML_Utils "
                                     "package into a single torch script model which can be used for inference in"
                                     " in C++ codes")

    # Add mutually_exclusive_group to either load a FlexMLP model or create on based on input
    parser.add_argument('checkpoints', nargs='+',
                        help='Checkpoint files (.ckpt) that will be assembled into one model')
    parser.add_argument('--output', '-o', dest='output', required=False, default='AssemblyModel.pt',
                        help='file to which assembled model is saved in torch script format')
    parser.add_argument('--onnx', dest='onnx', required=False,
                        help='Save model to onnx format. (default: AssemblyModel.onnx')
    parser.add_argument('--scale-flag', '-s', action="store_true", default=False, dest='scale',
                        help='enables limit_scale flag for Assembly model. It bounds model inputs to 0 and 1')

    return parser.parse_args()


def main():
    args = parseArguments()

    # get model class, default: LightningFlexMLP
    model_class = getattr(stfs_pytoolbox.ML_Utils.models, args.type)

    # create list to save scalar information
    ymin = []
    ymax = []
    models = []

    for file in args.checkpoints:
        model = load_model_from_checkpoint(file)
        ymin.append(model.hparams.lparams.y_scaler.data_min_)
        ymax.append(model.hparams.lparams.y_scaler.data_max_)
        models.append(model.eval())

    # feature scaler is only required once and can be taken from the last loaded model
    xmin = model.hparams.lparams.x_scaler.data_min_
    xmax = model.hparams.lparams.x_scaler.data_max_

    # Converting list into numpy ndarrays
    ymin = np.asarray(ymin)
    ymax = np.asarray(ymax)

    if args.scale:
        limit = False

    with torch.no_grad():
        assemblyModel = AssemblyModel(models, xmin, xmax, ymin, ymax, limit_scale=args.scale)
        assemblyModel.toTorchScript(args.output)

    # As of now and pyTorchLighnting 0.7.6 has a bug where a pl.LightningModule cannot be converted using
    # torch.jit.script(model)
    # This was later fixed see: https://github.com/PyTorchLightning/pytorch-lightning/pull/2657
    # when using a never pytorchLightning version please don't use toTorchScript method anymore but the code below
    #torchscript = torch.jit.script(assemblyModel)
    #torchscript.save(args.output)

    if args.onnx:
        assemblyModel.to_onnx(args.onnx)


if __name__ == "__main__":
    main()
